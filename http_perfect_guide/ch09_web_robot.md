# 웹 로봇

## 1. 웹 네트워크는 단방향 Graph 구조이다

크롤링 봇의 입장에 웹 구조는 일종의 그래프 자료구조와 유사합니다.  
문서 하나하나를 노드로 보고, 문서에서 문서로 이동하는 링크를 엣지로 볼 수 있겠습니다.

또한 웹 상의 문서는 수없이 많기 때문에 (수억개의 URL을 탐색해야할 수도 있다)
해시테이블이나 트리 자료구조 등의 자료구조 및 탐색 알고리즘을 잘 구축하는 것이 중요합니다. 

![Figure.9.1](https://kr.object.ncloudstorage.com/load0ne/http_perfect_guide/web_contents_network.png)

<br/>

## 2. 루프와 중복

그래프 구조를 탐색하는데 있어서 중요한 점으로 **루프와 중복**을 꼽을 수 있습니다.  
이는 크롤러 입장에서도, 웹서버 입장에서도 불필요한 트래픽을 지속적으로 발생시켜 불이익을 초래합니다.

서버 설계에 따라 순환구조가 발생할 수도 있고, 서버 개발자가 악의적으로 순환구조를 만들어 넣을 수도 있습니다.  
이를 개선하기 위한 키워드로 이 책에서는 다음의 방법들을 제시합니다.

![Figure](https://kr.object.ncloudstorage.com/load0ne/http_perfect_guide/infinite_crawling.png)

### 2.1. URL 정규화

중복을 피하기 위해 URL은 정규화 시킬 필요가 있습니다.  
예를 들면 `www.dropy.online` 과 `www.dropy.online#header` 두 URL 은 다른 문자열이지만 같은 리소스 주소를 가르킵니다.  
또 `www.dropy.online/~`, `www.dropy.online/%7F` 와 같이 인코딩으로 인해 같은 리소스가 다른 URL을 가지게 될 수도 있습니다.  
이 책에선 크게 다음과 같은 정규화가 가능하다고 다룹니다.

- 생략 가능한 포트번호
- URL 인코딩
- URL 앵커(#)
- 대소문자
- index.html
- IP 주소

### 2.2. BFS 크롤링

웹 네트워크는 그래프 구조이지만, 단방향 엣지와 크롤링 시작노드(루트노드)를 가진다는 점에서 트리구조로 볼 수도 있습니다.  
이때 DFS 방식으로 크롤링을 하게되면 특정 웹서버가 순환구조를 만들 때, 그 웹서버 순환에서 빠져나올 수 없게됩니다.

이를 BFS 방식으로 순환시킴으로써, 특정 웹서버가 순환구조를 만들고 있을 때도 다른 탐색에는 영향을 미치지 않도록 할 수 있습니다.

### 2.3. Throttle

하나의 웹서버에서 발생하는 순환구조를 해결하기 위해, 하나의 웹서버를 크롤링 하는 빈도를 감소시키던지, 횟수를 제한함으로써 대응 할 수 있습니다.

### 2.4. URL 크기 제한

만약 웹서버에서 링크를 순환구조로 설계했다면 무한하게 URL이 증가될 것입니다.  
예를 들어서 dropy 서버개발자가 악의적으로 무한히 순환하는 구조를 설계했다고 가정합시다.  
그럼 `www.dropy.online` > `www.dropy.online/wtf` > `www.dropy.online/wtf/wtf` > ...  
이렇게 계속해서 똑같은 html을 넘겨주기에 순환구조가 생기게 됩니다.  
이를 URL 크기를 제한함으로써 해결할 수 있습니다.

### 2.5. URL 패턴 발견

위와 같은 이유로 만약 탐색이 순환하게 된다면 알고리즘에 의한 순환이기에 특정 패턴이 발견될 수 밖에 없습니다.  
이런 패턴을 인지할 수 있는 알고리즘을 만들어 해결해야 합니다.

### 2.6. URL 블랙리스트

이미 악의적이거나 순환구조로 판명난 웹서버에 대해서는 블랙리스트로 지정하여 더이상 크롤링을 진행하지 않도록 합니다.

### 2.7. 컨텐츠 지문(fingerprint)

해당 컨텐츠에서 몇개의 바이트를 얻어낸 후, 체크섬을 계산하고 이 체크섬을 통해 중복성 체크를 하는 방식입니다.

### 2.8. 사람의 모니터링

위의 방식 뿐 아니라 다양한 방식으로 개선을 한다고 해도, 알고리즘이 안전하다는 것을 보장할 수는 없습니다.  
따라서 로봇에 대한 로그데이터 등을 수집하며 항상 사람이 모니터링하는 시스템이 필요합니다.

<br/>

## 3. robot.txt

웹서버 측에서 로봇들의 행동을 제어할 수 있는 표준으로 사용됩니다.  
이 파일은 항상 루트에 위치하도록 합니다. (ex. `www.dropy.online/robot.txt`)

버전이 올라감에 따라 더 여러 필드를 제공하고 있는데,  
이 책에선 우선 기본적으로 사용되는 `User-Agent`, `Allow`, `Disallow` 등을 다룹니다.
사용은 간단하기에 아래 예시를 보면 알 수 있습니다.

```none
# 샵으로 시작하는 라인은 주석입니다.
# 여기에 명시되지 않은 Agent 들은 모두 허용합니다.
# 아래는 webcrawler 라는 Agent 에게 /private 서버트리를 허용하지 않겠다는 뜻입니다.

User-Agent: webcrawler
Disallow: /private
```

<br/>

## 4. 메타 태그

웹 서버는 컨텐츠를 제공할 때, 메타테그를 잘 입력해야합니다.  
이는 로봇들이 읽고 자주 활용하는 데이터 이기 때문입니다.

### 4.1. 정보 메타태그

이는 로봇에게 현재 컨텐츠에 대한 정보를 전달합니다.

```html
<meta name="description" content="Dropy 온라인 프레젠테이션 서비스입니다">
<meta name="keywords" content="Presentation">
```

### 4.2. 제어 메타태그

`robot.txt`와 같이 현재 컨텐츠에 대한 로봇의 동작을 제어합니다.

```html
<!-- 현재 페이지를 처리하지마라 -->
<meta name="robots" content="noindex">
<!-- 현재 페이지에서 연결된 페이지들을 처리하지 마라 -->
<meta name="robots" content="nofollow">
<!-- noindex 와 반대 -->
<meta name="robots" content="index">
<!-- nofollow 와 반대 -->
<meta name="robots" content="follow">
<!-- index + follow -->
<meta name="robots" content="all">
<!-- noindex + nofollow -->
<meta name="robots" content="none">
```

## 5. 로봇 에티켓

1993년 마틴 코스터는 웹 로봇을 만드는 사람들을 위한 가이드라인을 작성했습니다.  
한번 정도 책에 있는 자세한 내용을 볼것을 권하며 간단히 리스팅해보았습니다.

### 5.1. 신원 식별

- 로봇의 신원을 밝히라
- 기계의 신원을 밝히라
- 연락처를 발히라

### 5.2. 동작

- 긴장하라
- 대비하라
- 감시와 로그
- 배우고 조정하라

### 5.3. 스스로를 제한하라

- URL을 필터링하라
- 동적 URL을 필터링하라
- Accept 관련 헤더로 필터링하라
- robots.txt에 따르라
- 스스로를 억제하라

### 5.4. 루프와 중복을 견뎌내기, 그 외 문제들

- 모든 응답코드 다루기
- URL 정규화하기
- 함정을 감시하라
- 블랙리스트를 관리하라

### 5.5. 확장성

- 공간 이해하기
- 대역폭 이해하기
- 시간 이해하기
- 분할 정복

### 5.6. 신뢰성

- 철저하게 테스트하라
- 체크포인트
- 실패에 대한 유연성

### 5.7. 소통

- 준비하라
- 이해하라
- 즉각 대응하라

